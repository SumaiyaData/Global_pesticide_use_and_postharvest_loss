# -*- coding: utf-8 -*-
"""Agri-Business Dashboard: Pesticide Use vs. Postharvest Loss

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jkWVZdUTtZpGyEEV2ho4vFTYOhIVKWr1
"""

#1. Load necessary Libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os

# Set your project folder
base_path = r"E:\All Projects\postharvest loss"

# Build file paths
america_path = os.path.join(base_path, "Inputs_Pesticides_Use_E_Americas_NOFLAG.csv")
asia_path = os.path.join(base_path, "Inputs_Pesticides_Use_E_Asia_NOFLAG.csv")
europe_path = os.path.join(base_path, "Inputs_Pesticides_Use_E_Europe_NOFLAG.csv")
mapping_path = os.path.join(base_path, "UNSD — Methodologyy.csv")
postharvest_path = os.path.join(base_path, "food-loss-postharvest-by-region.csv")  

# 2. Load datasets
america_original = pd.read_csv(america_path, encoding="cp1252")
asia_original = pd.read_csv(asia_path, encoding="cp1252")
europe_original = pd.read_csv(europe_path, encoding="cp1252")
mapping_original = pd.read_csv(mapping_path, encoding="cp1252")
postharvest_original = pd.read_csv(postharvest_path, encoding="cp1252")

# 3. Always make a copy of the original file

america= america_original.copy()
asia= asia_original.copy()
europe= europe_original.copy()
mapping= mapping_original.copy()
postharvest_loss= postharvest_original.copy()

# 4. Reshape data: Wide → Long

america = pd.melt(america, id_vars=['Area Code', 'Area Code (M49)','Area', 'Item Code', 'Item', 'Element Code', 'Element', 'Unit'], value_vars=[col for col in america.columns if col.startswith('Y')], value_name='value', var_name='Year')
asia = pd.melt(asia, id_vars=['Area Code', 'Area Code (M49)','Area', 'Item Code', 'Item', 'Element Code', 'Element', 'Unit'], value_vars=[col for col in asia.columns if col.startswith('Y')], value_name='value', var_name='Year')
europe = pd.melt(europe, id_vars=['Area Code', 'Area Code (M49)','Area', 'Item Code', 'Item', 'Element Code', 'Element', 'Unit'], value_vars=[col for col in europe.columns if col.startswith('Y')], value_name='value', var_name='Year')

# 5. Adjust Names

america['Year'] = america['Year'].str.replace("Y", "").astype(int)
asia['Year'] = asia['Year'].str.replace('Y', "").astype(int)
europe['Year'] = europe['Year'].str.replace('Y', "").astype(int)
postharvest_loss.rename(columns={'12.3.1a Food loss percentage | 000000000024044 || Value | 006121 || percent': 'Food loss percentage'}, inplace= True)
postharvest_loss.rename(columns={'Entity':'Sub-region Name'}, inplace=True)

# 6. combine and save these three tables (stacking by Vertical Combination) and edit the final table

all_continents_data = pd.concat([america, europe, asia], ignore_index=True)
all_continents_data['Area'] = all_continents_data['Area'].replace({'Palestine': 'State of Palestine',
    'China, Hong Kong SAR': 'China, Hong Kong Special Administrative Region',
    'China, Macao SAR': 'China, Macao Special Administrative Region'})

all_continents_data['Area Code (M49)'] = all_continents_data['Area Code (M49)'].astype(str).str.replace("'", "")

all_continents_data.to_csv('all_data.csv', index=False)
all_continents_data.head(10)


# 7. Check whether the combination has successfully occured or not

print(europe.shape)
print(asia.shape)
print(america.shape)
print(all_continents_data.shape)
print(45118 + 32844 + 30566)


# 8. format mapping dataframe

mapping= mapping.drop(columns=['Global Code', 'Global Name', 'Region Code', 'Region Name', 'Intermediate Region Code',
                               'Intermediate Region Name', 'ISO-alpha2 Code', 'ISO-alpha3 Code', 'Least Developed Countries (LDC)',
                               'Land Locked Developing Countries (LLDC)', 'Small Island Developing States (SIDS)', 'Sub-region Code'])

mapping.rename(columns={'Country or Area':'Area'}, inplace=True)
mapping.rename(columns={'M49 Code':'Area Code (M49)'}, inplace=True)
mapping.to_csv('mapping.csv', index=False)


# 9. Merge mapping and all_continents_data

all_continents_data['Area Code (M49)'] = all_continents_data['Area Code (M49)'].astype(str).str.strip()
mapping['Area Code (M49)'] = mapping['Area Code (M49)'].astype(str).str.strip()
all_continents_data['Area'] = all_continents_data['Area'].str.strip()
mapping['Area'] = mapping['Area'].str.strip()
all_data = all_continents_data.merge(
    mapping[['Area Code (M49)', 'Area', 'Sub-region Name']],
    on=['Area Code (M49)', 'Area'],
    how='left'
)

# Check how many still have NaN
print(all_data['Sub-region Name'].isna().sum())

# Assign manually for remaining unmatched countries
manual_map = {
"Antigua and Barbuda": "Latin America and the Caribbean",
    "Argentina": "Latin America and the Caribbean",
    "Bahamas": "Latin America and the Caribbean",
    "Barbados": "Latin America and the Caribbean",
    "Belize": "Latin America and the Caribbean",
    "Bermuda": "Northern America",
    "Bolivia (Plurinational State of)": "Latin America and the Caribbean",
    "Brazil": "Latin America and the Caribbean",
    "British Virgin Islands": "Latin America and the Caribbean",
    "China, Taiwan Province of": "Eastern Asia",
    "China, mainland": "Eastern Asia",
    "Brunei Darussalam": "South-eastern Asia",
    "Bhutan": "Southern Asia",
    "Bangladesh": "Southern Asia",
    "Bahrain": "Western Asia",
    "Azerbaijan": "Western Asia",
    "Armenia": "Western Asia",
    "Brunei Darussalam": "South-eastern Asia",
    "Albania":"Southern Europe",
    "Serbia and Montenegro":"Southern Europe",
    "Bosnia and Herzegovina":"Southern Europe",
    "Andorra":"Southern Europe",
    "Yugoslav SFR":"Southern Europe",
    "Belgium-Luxembourg":"Western Europe",
    "Belgium":"Western Europe",
    "Austria":"Western Europe"}

all_data['Sub-region Name'] = all_data.apply(
    lambda row: manual_map[row['Area']] if pd.isna(row['Sub-region Name']) and row['Area'] in manual_map else row['Sub-region Name'],
    axis=1)


# Show rows where 'Sub-region Name' is NaN
empty_subregions = all_data[all_data['Sub-region Name'].isna()]
print(empty_subregions)

# Empty strings
empty_or_blank = all_data[all_data['Sub-region Name'].isna() | (all_data['Sub-region Name'].str.strip() == "")]
print(empty_or_blank)

# List of defunct countries
defunct_countries = [
    "Netherlands Antilles (former)",
    "USSR",
    "Czechoslovakia"
]

# Drop rows
all_data = all_data[~all_data['Area'].isin(defunct_countries)]

# Check if any NaN or blank values remain

missing = all_data['Sub-region Name'].isna() | (all_data['Sub-region Name'].str.strip() == "")
print("Any missing sub-region names?", missing.any())
all_data.to_csv('all_data.csv', index=False)

# Replace empty strings with NaN in 'value' column only

all_data['value'] = all_data['value'].replace(r'^\s*$', np.nan, regex=True)

# Drop rows where 'value' is NaN or 0
all_data = all_data.dropna(subset=['value'])
all_data = all_data[all_data['value'] != 0]

# Check if any 0 values remain
zeros_left = (all_data['value'] == 0).sum()

# Check if any empty strings remain
empty_left = (all_data['value'] == "").sum()

# Check if any NaN values remain
nan_left = all_data['value'].isna().sum()

print("Remaining zeros:", zeros_left)
print("Remaining empty strings:", empty_left)
print("Remaining NaNs:", nan_left)

# Count NaNs per column
print(all_data.isna().sum())

# Count total NaNs in the whole dataframe
print("Total NaNs:", all_data.isna().sum().sum())

#from all_data delete area code, item code, element code,

all_data = all_data.drop(columns=['Area Code', 'Item Code', 'Element Code'])

all_data_without_postharvest = all_data.copy()
all_data_without_postharvest.to_csv('all_data_without_postharvest.csv', index= False)



# 10. Graphical analysis
# Exploratory Analysis — Pesticide trends (1990–2023)



# 10A: Global total pesticide use time series (tons)

print("----------Global total pesticide use time series (tons)--------")
total_use = all_data_without_postharvest[
    (all_data_without_postharvest["Element"].str.contains("Agricultural Use", case=False, na=False)) &
    (all_data_without_postharvest["Unit"] == "t")
].copy()

global_use_in_tons = total_use.groupby('Year')['value'].sum().reset_index()
plt.figure(figsize=(12,6))
sns.lineplot(data=global_use_in_tons, x='Year', y='value', marker='o')
plt.title('Global pesticide use (tons) 1990-2023')
plt.ylabel('Tons')
plt.xlabel('Year')
plt.grid(alpha=0.3)
plt.show()
global_use_in_tons.to_csv('global_ts.csv', index= False)


# 10B: Sub-region trends

print("----------Top sub-region pesticide use (tons) 1990-2023--------")
region_trends = total_use.groupby(['Year','Sub-region Name'])['value'].sum().reset_index()
cum_by_region = region_trends.groupby('Sub-region Name')['value'].sum().sort_values(ascending=False)
top_regions = cum_by_region.head(8).index.tolist()

plt.figure(figsize=(12,7))
for r in top_regions:
    d = region_trends[region_trends['Sub-region Name']==r]
    plt.plot(d['Year'], d['value'], marker='o', label=r)
plt.title('Top sub-region pesticide use (tons) 1990-2023')
plt.ylabel('Tons')
plt.legend(bbox_to_anchor=(1.02,1))
plt.show()
region_trends.to_csv('region_trends.csv', index=False)


# 10C: Pesticide type stacked bar (Herbicides, Insecticides, Fungicides) - milestone years

types_df = all_data_without_postharvest[
    (all_data_without_postharvest['Element'].str.contains('Agricultural Use', case=False, na=False)) &
    (all_data_without_postharvest['Item'] != 'Pesticides (total)') &
    (all_data_without_postharvest['Unit'] == 't')
].copy()

# Choose milestone years for

# a. Simplifies visualization: Plotting 30+ years stacked bars can be very cluttered.
# b. focusing on periodical change can potray entire picture

milestones = [1990, 2000, 2010, 2015, 2020, 2023]
types_sel = types_df[types_df['Year'].isin(milestones)]
type_pivot = types_sel.groupby(['Year','Item'])['value'].sum().unstack(fill_value=0)

type_pivot.plot(kind='bar', stacked=True, figsize=(14,6))
plt.title('Pesticide use by type (tons) — milestone years')
plt.ylabel('Tons')
plt.xlabel('Year')
plt.legend(title='Pesticide Type', bbox_to_anchor=(1.02,1))
plt.show()

type_pivot.to_csv('type_pivot.csv')

# 11. Now merge and Analyze on postharvest loss data
# Prepare postharvest data
# Lowercase, strip, remove "(FAO)" and normalize whitespace
postharvest_loss['Sub-region Name'] = postharvest_loss['Sub-region Name'].astype(str).str.lower().str.strip()
postharvest_loss['Sub-region Name'] = postharvest_loss['Sub-region Name'].str.replace(r'\(fao\)', '', regex=True).str.strip()
postharvest_loss['Sub-region Name'] = postharvest_loss['Sub-region Name'].str.replace(r'\s+', ' ', regex=True)

# Remove aggregate/combined groups that are not needed
remove_list = [
    "eastern asia and south-eastern asia",
    "land locked developing countries",
    "least developed countries",
    "northern america and europe",
    "small island developing states",
    "world"
]
postharvest_loss = postharvest_loss[~postharvest_loss['Sub-region Name'].isin(remove_list)].copy()

print("Postharvest (unique sub-regions):", postharvest_loss['Sub-region Name'].unique())
postharvest_loss = postharvest_loss.drop(columns=['Code'])

# Standardize all_data Sub-region for matching: lowercase + strip

all_data_without_postharvest['Sub-region Name'] = all_data_without_postharvest['Sub-region Name'].astype(str).str.lower().str.strip()
all_data_without_postharvest['Sub-region Name'] = all_data_without_postharvest['Sub-region Name'].str.replace(r'\s+', ' ', regex=True)

# Merge on sub-region name + year to attach 'Food loss percentage'
complete_data_with_postharvest_loss = all_data_without_postharvest.merge(
    postharvest_loss[['Sub-region Name', 'Year', 'Food loss percentage']],
    on=['Sub-region Name', 'Year'],
    how='left',
    validate='m:1'  # many pesticide rows can map to one PHL row
)

print("Merged pesticide + postharvest. Shape:", complete_data_with_postharvest_loss.shape)
print("Rows where Food loss percentage is present:", complete_data_with_postharvest_loss['Food loss percentage'].notna().sum())

# Save merged pre-filtered dataset
complete_data_with_postharvest_loss.head(10)

complete_data_with_postharvest_loss = complete_data_with_postharvest_loss.dropna(subset=['Food loss percentage']).copy()  # keep only rows with PHL (2016,2020,2021)

print("PHL-overlap dataset rows:", len(complete_data_with_postharvest_loss))
print("PHL years present:", sorted(complete_data_with_postharvest_loss['Year'].unique()))

complete_data_with_postharvest_loss.to_csv('complete_data_with_postharvest_loss.csv', index=False)


# 12. Graphical analysis


# Basic scatter: pesticide vs PHL (color by sub-region)
plt.figure(figsize=(10,6))
sns.scatterplot(data=complete_data_with_postharvest_loss, x='value', y='Food loss percentage', hue='Sub-region Name', alpha=0.8)
plt.xscale('log')  # log scale often helps visualize skewed pesticide totals
plt.xlabel('Pesticide use (value) [log scale]')
plt.ylabel('Postharvest loss (%)')
plt.title('Pesticide use vs Postharvest loss (2016,2020,2021)')
plt.legend(bbox_to_anchor=(1.02,1))
plt.show()

# Correlation summary
corr = complete_data_with_postharvest_loss[['value','Food loss percentage']].corr().round(3)
print("Correlation matrix (value vs Food loss percentage):")
print(corr)


# Regional averages (PHL overlap)
regional_eff = complete_data_with_postharvest_loss.groupby('Sub-region Name')[['value','Food loss percentage']].mean().reset_index().sort_values('value', ascending=False)
# regional_eff.head(12).to_csv('/content/outputs/regional_efficiency_summary.csv', index=False)
print(regional_eff.head(12))


# Efficiency quadrants using medians (PHL overlap)
p_med = complete_data_with_postharvest_loss['value'].median()
l_med = complete_data_with_postharvest_loss['Food loss percentage'].median()
print(f"Pesticide median: {p_med:.2f}, Food-loss median: {l_med:.2f}")

def efficiency_quadrant(r):
    if r['value'] >= p_med and r['Food loss percentage'] < l_med:
        return 'Efficient (High pesticide, Low loss)'
    if r['value'] >= p_med and r['Food loss percentage'] >= l_med:
        return 'Over-spenders (High pesticide, High loss)'
    if r['value'] < p_med and r['Food loss percentage'] < l_med:
        return 'Hidden Gems (Low pesticide, Low loss)'
    return 'Vulnerable (Low pesticide, High loss)'

complete_data_with_postharvest_loss['Efficiency Group'] = complete_data_with_postharvest_loss.apply(efficiency_quadrant, axis=1)

plt.figure(figsize=(10,6))
sns.scatterplot(data=complete_data_with_postharvest_loss, x='value', y='Food loss percentage', hue='Efficiency Group', alpha=0.8)
plt.xscale('log')
plt.axvline(p_med, color='k', linestyle='--')
plt.axhline(l_med, color='k', linestyle='--')
plt.title('Efficiency Quadrants (PHL overlap)')
plt.show()


# Efficiency ratio for sub-regions: lower is better (pesticide per unit secured)
complete_data_with_postharvest_loss['Efficiency_Ratio'] = complete_data_with_postharvest_loss['value'] / (100 - complete_data_with_postharvest_loss['Food loss percentage'])
sub_eff = complete_data_with_postharvest_loss.groupby('Sub-region Name')['Efficiency_Ratio'].mean().sort_values()
# sub_eff.head(15).to_csv('/content/outputs/subregion_efficiency_rank.csv')
print(sub_eff.head(15))


## 16. Temporal view: 2016 vs 2020 vs 2021 (aggregate)
# Show how pesticide use and postharvest loss changed across available years.


yearly = complete_data_with_postharvest_loss.groupby('Year')[['value','Food loss percentage']].mean().reset_index()
print(yearly)

plt.figure(figsize=(10,6))
plt.plot(yearly['Year'], yearly['value'], marker='o', label='Avg pesticide (value)')
plt.plot(yearly['Year'], yearly['Food loss percentage'], marker='o', label='Avg Food loss %')
plt.title('Average pesticide & postharvest loss (PHL overlap years)')
plt.ylabel('Average')
plt.legend()
plt.show()


## 17. COVID-19 impact inspection (2020-2021 vs 2016)
# We can compare the overlap years to get a rough sense of pandemic impacts on efficiency.

covid_compare = complete_data_with_postharvest_loss.groupby('Year')[['value','Food loss percentage']].mean()
print(covid_compare)

# Bar chart
covid_compare.plot(kind='bar', subplots=True, layout=(1,2), figsize=(14,5), legend=False)
plt.suptitle('PHL-overlap: pesticide vs postharvest loss by year')
plt.show()


# 18. Aggregate pesticide use by decade
all_data["Decade"] = (all_data["Year"] // 10) * 10
decade_trends = all_data.groupby("Decade")["value"].sum().reset_index()

# Growth rate between decades
decade_trends["GrowthRate"] = decade_trends["value"].pct_change() * 100

# Plot
sns.lineplot(data=decade_trends, x="Decade", y="value", marker="o")
plt.title("Total Pesticide Use by Decade")
plt.ylabel("Total Use")
plt.show()

print(decade_trends)


# 19. Breakpoints (Policy/Crises Approximation)

# Year-over-year % change
region_trends = all_data.groupby(["Year"])["value"].sum().reset_index()
region_trends["YoY_Change"] = region_trends["value"].pct_change() * 100

# Show top 5 jumps/drops
print("Major Turning Points (YoY Changes)")
print(region_trends.nlargest(5, "YoY_Change")[["Year","YoY_Change"]])
print(region_trends.nsmallest(5, "YoY_Change")[["Year","YoY_Change"]])


# 20. Top 10 Pesticide Users (Volume)

top10_volume = all_data.groupby("Area")["value"].sum().nlargest(10)
top10_volume.plot(kind="bar", color="teal")
plt.title("Top 10 Countries by Pesticide Volume (1990–Present)")
plt.ylabel("Total Pesticide Use")
plt.show()


# 21. 2023 Global Landscape

landscape_2023 = all_data[all_data["Year"] == 2023].groupby("Area")["value"].sum().nlargest(15)
landscape_2023.plot(kind="bar", color="orange")
plt.title("2023 Pesticide Use by Country (Top 15)")
plt.ylabel("Use (Total Value)")
plt.show()