# -*- coding: utf-8 -*-
"""agri_business_dashboard_pesticide_use_vs_postharvest_loss

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jkWVZdUTtZpGyEEV2ho4vFTYOhIVKWr1
"""

# Load necessary Libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os

# Load Data Sets

america_original = pd.read_csv('/content/Inputs_Pesticides_Use_E_Americas_NOFLAG.csv')
asia_original= pd.read_csv('/content/Inputs_Pesticides_Use_E_Asia_NOFLAG.csv')
europe_original= pd.read_csv('/content/Inputs_Pesticides_Use_E_Europe_NOFLAG.csv')
mapping_original = pd.read_csv('/content/UNSD â€” Methodologyy.csv', encoding='cp1252')
postharvest_loss_original= pd.read_csv('/content/food-loss-postharvest-by-region.csv')

# Always make a copy of the original file

america= america_original.copy()
asia= asia_original.copy()
europe= europe_original.copy()
mapping= mapping_original.copy()
postharvest_loss= postharvest_loss_original.copy()

# Reshape data: Wide â†’ Long

america = pd.melt(america, id_vars=['Area Code', 'Area Code (M49)','Area', 'Item Code', 'Item', 'Element Code', 'Element', 'Unit'], value_vars=[col for col in america.columns if col.startswith('Y')], value_name='value', var_name='Year')
asia = pd.melt(asia, id_vars=['Area Code', 'Area Code (M49)','Area', 'Item Code', 'Item', 'Element Code', 'Element', 'Unit'], value_vars=[col for col in asia.columns if col.startswith('Y')], value_name='value', var_name='Year')
europe = pd.melt(europe, id_vars=['Area Code', 'Area Code (M49)','Area', 'Item Code', 'Item', 'Element Code', 'Element', 'Unit'], value_vars=[col for col in europe.columns if col.startswith('Y')], value_name='value', var_name='Year')

# Adjust Names

america['Year'] = america['Year'].str.replace("Y", "").astype(int)
asia['Year'] = asia['Year'].str.replace('Y', "").astype(int)
europe['Year'] = europe['Year'].str.replace('Y', "").astype(int)
postharvest_loss.rename(columns={'12.3.1a Food loss percentage | 000000000024044 || Value | 006121 || percent': 'Food loss percentage'}, inplace= True)
postharvest_loss.rename(columns={'Entity':'Sub-region Name'}, inplace=True)


# Combine and save these three tables (stacking by Vertical Combination) and edit the final table

all_continents_data = pd.concat([america, europe, asia], ignore_index=True)
all_continents_data['Area'] = all_continents_data['Area'].replace({'Palestine': 'State of Palestine',
    'China, Hong Kong SAR': 'China, Hong Kong Special Administrative Region',
    'China, Macao SAR': 'China, Macao Special Administrative Region'})

all_continents_data['Area Code (M49)'] = all_continents_data['Area Code (M49)'].astype(str).str.replace("'", "")
all_continents_data.to_csv('all_continents_data.csv', index=False)


# Check whether the combination has successfully occured or not

print(europe.shape)
print(asia.shape)
print(america.shape)
print(all_continents_data.shape)
print(45118 + 32844 + 30566)


# Format mapping dataframe

mapping= mapping.drop(columns=['Global Code', 'Global Name', 'Region Code', 'Region Name', 'Intermediate Region Code',
                               'Intermediate Region Name', 'ISO-alpha2 Code', 'ISO-alpha3 Code', 'Least Developed Countries (LDC)',
                               'Land Locked Developing Countries (LLDC)', 'Small Island Developing States (SIDS)', 'Sub-region Code'])

mapping.rename(columns={'Country or Area':'Area'}, inplace=True)
mapping.rename(columns={'M49 Code':'Area Code (M49)'}, inplace=True)
mapping.to_csv('mapping.csv', index=False)


# Merge mapping and all_continents_data

all_continents_data['Area Code (M49)'] = all_continents_data['Area Code (M49)'].astype(str).str.strip()
mapping['Area Code (M49)'] = mapping['Area Code (M49)'].astype(str).str.strip()
all_continents_data['Area'] = all_continents_data['Area'].str.strip()
mapping['Area'] = mapping['Area'].str.strip()
all_data = all_continents_data.merge(
    mapping[['Area Code (M49)', 'Area', 'Sub-region Name']],
    on=['Area Code (M49)', 'Area'],
    how='left'
)


# Check how many still have NaN
print(all_data['Sub-region Name'].isna().sum())

# Assign manually for remaining unmatched countries
manual_map = {
"Antigua and Barbuda": "Latin America and the Caribbean",
    "Argentina": "Latin America and the Caribbean",
    "Bahamas": "Latin America and the Caribbean",
    "Barbados": "Latin America and the Caribbean",
    "Belize": "Latin America and the Caribbean",
    "Bermuda": "Northern America",
    "Bolivia (Plurinational State of)": "Latin America and the Caribbean",
    "Brazil": "Latin America and the Caribbean",
    "British Virgin Islands": "Latin America and the Caribbean",
    "China, Taiwan Province of": "Eastern Asia",
    "China, mainland": "Eastern Asia",
    "Brunei Darussalam": "South-eastern Asia",
    "Bhutan": "Southern Asia",
    "Bangladesh": "Southern Asia",
    "Bahrain": "Western Asia",
    "Azerbaijan": "Western Asia",
    "Armenia": "Western Asia",
    "Brunei Darussalam": "South-eastern Asia",
    "Albania":"Southern Europe",
    "Serbia and Montenegro":"Southern Europe",
    "Bosnia and Herzegovina":"Southern Europe",
    "Andorra":"Southern Europe",
    "Yugoslav SFR":"Southern Europe",
    "Belgium-Luxembourg":"Western Europe",
    "Belgium":"Western Europe",
    "Austria":"Western Europe"}

all_data['Sub-region Name'] = all_data.apply(
    lambda row: manual_map[row['Area']] if pd.isna(row['Sub-region Name']) and row['Area'] in manual_map else row['Sub-region Name'],
    axis=1)


# Show rows where 'Sub-region Name' is NaN

empty_subregions = all_data[all_data['Sub-region Name'].isna()]
print(empty_subregions)

# Empty strings

empty_or_blank = all_data[all_data['Sub-region Name'].isna() | (all_data['Sub-region Name'].str.strip() == "")]
print(empty_or_blank)

# List of defunct countries

defunct_countries = [
    "Netherlands Antilles (former)",
    "USSR",
    "Czechoslovakia"
]

# Drop rows

all_data = all_data[~all_data['Area'].isin(defunct_countries)]

# Check if any NaN or blank values remain

missing = all_data['Sub-region Name'].isna() | (all_data['Sub-region Name'].str.strip() == "")
print("Any missing sub-region names?", missing.any())
all_data.to_csv('all_data.csv', index=False)

# Replace empty strings with NaN in 'value' column only

all_data['value'] = all_data['value'].replace(r'^\s*$', np.nan, regex=True)

# Drop rows where 'value' is NaN or 0
all_data = all_data.dropna(subset=['value'])
all_data = all_data[all_data['value'] != 0]

# Check if any 0 values remain
zeros_left = (all_data['value'] == 0).sum()

# Check if any empty strings remain
empty_left = (all_data['value'] == "").sum()

# Check if any NaN values remain
nan_left = all_data['value'].isna().sum()

print("Remaining zeros:", zeros_left)
print("Remaining empty strings:", empty_left)
print("Remaining NaNs:", nan_left)

# Count NaNs per column
print(all_data.isna().sum())

# Count total NaNs in the whole dataframe

print("Total NaNs:", all_data.isna().sum().sum())

# From all_data delete area code, item code, element code

all_data = all_data.drop(columns=['Area Code', 'Item Code', 'Element Code'])
all_data_without_postharvest = all_data.copy()
all_data_without_postharvest.to_csv('all_data_without_postharvest.csv', index= False)


# All graphical analysis
# 1. Global total pesticide use time series (tons)

print("----------Global total pesticide use time series (tons)--------")
total_use = all_data_without_postharvest[
    (all_data_without_postharvest["Element"].str.contains("Agricultural Use", case=False, na=False)) &
    (all_data_without_postharvest["Unit"] == "t")
].copy()

global_use_in_tons = total_use.groupby('Year')['value'].sum().reset_index()
plt.figure(figsize=(12,6))
sns.lineplot(data=global_use_in_tons, x='Year', y='value', marker='o')
plt.title('Global pesticide use (tons) 1990-2023')
plt.ylabel('Tons')
plt.xlabel('Year')
plt.grid(alpha=0.3)
plt.show()


# 2. Sub-region trends

print("----------Top 8 sub-region pesticide use (tons) 1990-2023--------")

region_trends = total_use.groupby(['Year','Sub-region Name'])['value'].sum().reset_index()
cum_by_region = region_trends.groupby('Sub-region Name')['value'].sum().sort_values(ascending=False)
top_regions = cum_by_region.head(8).index.tolist()

plt.figure(figsize=(12,7))
for r in top_regions:
    d = region_trends[region_trends['Sub-region Name']==r]
    plt.plot(d['Year'], d['value'], marker='o', label=r)
plt.title('Top 8 sub-region pesticide use (tons) 1990-2023')
plt.ylabel('Tons')
plt.legend(bbox_to_anchor=(1.02,1))
plt.show()
print("Top 8 sub-regions by pesticide use:", top_regions)


# 3. Pesticide type stacked bar (Herbicides, Insecticides, Fungicides) - milestone years

types_df = all_data_without_postharvest[
    (all_data_without_postharvest['Element'].str.contains('Agricultural Use', case=False, na=False)) &
    (all_data_without_postharvest['Item'] != 'Pesticides (total)') &
    (all_data_without_postharvest['Unit'] == 't')
].copy()

# Choose milestone years for:
# a. Simplifies visualization: Plotting 30+ years stacked bars can be very cluttered.
# b. focusing on periodical change can potray entire picture

milestones = [1990, 2000, 2010, 2015, 2020, 2023]
types_sel = types_df[types_df['Year'].isin(milestones)]
type_pivot = types_sel.groupby(['Year','Item'])['value'].sum().unstack(fill_value=0)
type_pivot.plot(kind='bar', stacked=True, figsize=(14,6))
plt.title('Pesticide use by type (tons) â€” milestone years')
plt.ylabel('Tons')
plt.xlabel('Year')
plt.legend(title='Pesticide Type', bbox_to_anchor=(1.02,1))
plt.show()


# Now merge and Analyze on postharvest loss data

postharvest_loss['Sub-region Name'] = postharvest_loss['Sub-region Name'].astype(str).str.lower().str.strip()
postharvest_loss['Sub-region Name'] = postharvest_loss['Sub-region Name'].str.replace(r'\(fao\)', '', regex=True).str.strip()
postharvest_loss['Sub-region Name'] = postharvest_loss['Sub-region Name'].str.replace(r'\s+', ' ', regex=True)
remove_list = [
    "eastern asia and south-eastern asia",
    "land locked developing countries",
    "least developed countries",
    "northern america and europe",
    "small island developing states",
    "world"
]
postharvest_loss = postharvest_loss[~postharvest_loss['Sub-region Name'].isin(remove_list)].copy()
print("Postharvest (unique sub-regions):", postharvest_loss['Sub-region Name'].unique())
postharvest_loss = postharvest_loss.drop(columns=['Code'])


# Standardize all_data Sub-region for matching: lowercase + strip

all_data_without_postharvest['Sub-region Name'] = all_data_without_postharvest['Sub-region Name'].astype(str).str.lower().str.strip()
all_data_without_postharvest['Sub-region Name'] = all_data_without_postharvest['Sub-region Name'].str.replace(r'\s+', ' ', regex=True)


# Merge on sub-region name + year to attach 'Food loss percentage'
complete_data_with_postharvest_loss = all_data_without_postharvest.merge(
    postharvest_loss[['Sub-region Name', 'Year', 'Food loss percentage']],
    on=['Sub-region Name', 'Year'],
    how='left',
    validate='m:1'  # many pesticide rows can map to one PHL row
)

print("Merged pesticide + postharvest. Shape:", complete_data_with_postharvest_loss.shape)
print("Rows where Food loss percentage is present:", complete_data_with_postharvest_loss['Food loss percentage'].notna().sum())

# Save merged pre-filtered dataset

complete_data_with_postharvest_loss = complete_data_with_postharvest_loss.dropna(subset=['Food loss percentage']).copy()  # keep only rows with PHL (2016,2020,2021)

print("PHL-overlap dataset rows:", len(complete_data_with_postharvest_loss))
print("PHL years present:", sorted(complete_data_with_postharvest_loss['Year'].unique()))

# Create the directory if it doesn't exist
if not os.path.exists('/content/outputs'):
    os.makedirs('/content/outputs')

complete_data_with_postharvest_loss.to_csv('complete_data_with_postharvest_loss.csv', index=False)


# Graphical analysis

# 4. Basic scatter: pesticide vs PHL (color by sub-region)

plt.figure(figsize=(10,6))
sns.scatterplot(data=complete_data_with_postharvest_loss, x='value', y='Food loss percentage', hue='Sub-region Name', alpha=0.8)
plt.xscale('log')  # log scale often helps visualize skewed pesticide totals
plt.xlabel('Pesticide use (value) [log scale]')
plt.ylabel('Postharvest loss (%)')
plt.title('Pesticide use vs Postharvest loss (2016,2020,2021)')
plt.legend(bbox_to_anchor=(1.02,1))
plt.show()

# Correlation summary
corr = complete_data_with_postharvest_loss[['value','Food loss percentage']].corr().round(3)
print("Correlation matrix (value vs Food loss percentage):")
print(corr)


# Regional averages (PHL overlap)
regional_eff = complete_data_with_postharvest_loss.groupby('Sub-region Name')[['value','Food loss percentage']].mean().reset_index().sort_values('value', ascending=False)
regional_eff.head(12).to_csv('/content/outputs/regional_efficiency_summary.csv', index=False)
display(regional_eff.head(12))


# Efficiency quadrants using medians (PHL overlap)
p_med = complete_data_with_postharvest_loss['value'].median()
l_med = complete_data_with_postharvest_loss['Food loss percentage'].median()
print(f"Pesticide median: {p_med:.2f}, Food-loss median: {l_med:.2f}")

def efficiency_quadrant(r):
    if r['value'] >= p_med and r['Food loss percentage'] < l_med:
        return 'Efficient (High pesticide, Low loss)'
    if r['value'] >= p_med and r['Food loss percentage'] >= l_med:
        return 'Over-spenders (High pesticide, High loss)'
    if r['value'] < p_med and r['Food loss percentage'] < l_med:
        return 'Hidden Gems (Low pesticide, Low loss)'
    return 'Vulnerable (Low pesticide, High loss)'

complete_data_with_postharvest_loss['Efficiency Group'] = complete_data_with_postharvest_loss.apply(efficiency_quadrant, axis=1)

plt.figure(figsize=(10,6))
sns.scatterplot(data=complete_data_with_postharvest_loss, x='value', y='Food loss percentage', hue='Efficiency Group', alpha=0.8)
plt.xscale('log')
plt.axvline(p_med, color='k', linestyle='--')
plt.axhline(l_med, color='k', linestyle='--')
plt.title('Efficiency Quadrants (PHL overlap)')
plt.show()


# Efficiency ratio for sub-regions: lower is better (pesticide per unit secured)
complete_data_with_postharvest_loss['Efficiency_Ratio'] = complete_data_with_postharvest_loss['value'] / (100 - complete_data_with_postharvest_loss['Food loss percentage'])
sub_eff = complete_data_with_postharvest_loss.groupby('Sub-region Name')['Efficiency_Ratio'].mean().sort_values()
sub_eff.head(15).to_csv('/content/outputs/subregion_efficiency_rank.csv')
display(sub_eff.head(15))
complete_data_with_postharvest_loss.to_csv ("complete_data_with_postharvest_loss.csv", index= False)


# Temporal view: 2016 vs 2020 vs 2021 (aggregate)
# Show how pesticide use and postharvest loss changed across available years.

yearly = complete_data_with_postharvest_loss.groupby('Year')[['value','Food loss percentage']].mean().reset_index()
print(yearly)
plt.figure(figsize=(10,6))
plt.plot(yearly['Year'], yearly['value'], marker='o', label='Avg pesticide (value)')
plt.plot(yearly['Year'], yearly['Food loss percentage'], marker='o', label='Avg Food loss %')
plt.title('Average pesticide & postharvest loss (PHL overlap years)')
plt.ylabel('Average')
plt.legend()
plt.show()


# COVID-19 impact inspection (2020-2021 vs 2016)
# We can compare the overlap years to get a rough sense of pandemic impacts on efficiency.

covid_compare = complete_data_with_postharvest_loss.groupby('Year')[['value','Food loss percentage']].mean()
print(covid_compare)
covid_compare.plot(kind='bar', subplots=True, layout=(1,2), figsize=(14,5), legend=False)
plt.suptitle('PHL-overlap: pesticide vs postharvest loss by year')
plt.show()


# Aggregate pesticide use by decade
all_data["Decade"] = (all_data["Year"] // 10) * 10
decade_trends = all_data.groupby("Decade")["value"].sum().reset_index()
decade_trends["GrowthRate"] = decade_trends["value"].pct_change() * 100
sns.lineplot(data=decade_trends, x="Decade", y="value", marker="o")
plt.title("Total Pesticide Use by Decade")
plt.ylabel("Total Use")
plt.show()
print(decade_trends)


# Breakpoints (Policy/Crises Approximation)

# Year-over-year % change
region_trends = all_data.groupby(["Year"])["value"].sum().reset_index()
region_trends["YoY_Change"] = region_trends["value"].pct_change() * 100

# Show top 5 jumps/drops
print("Major Turning Points (YoY Changes)")
print(region_trends.nlargest(5, "YoY_Change")[["Year","YoY_Change"]])
print(region_trends.nsmallest(5, "YoY_Change")[["Year","YoY_Change"]])


# Top 10 Pesticide Users (Volume)

top10_volume = all_data.groupby("Area")["value"].sum().nlargest(10)
top10_volume.plot(kind="bar", color="teal")
plt.title("Top 10 Countries by Pesticide Volume (1990â€“Present)")
plt.ylabel("Total Pesticide Use")
plt.show()


# 2023 Global Landscape

landscape_2023 = all_data[all_data["Year"] == 2023].groupby("Area")["value"].sum().nlargest(15)
landscape_2023.plot(kind="bar", color="orange")
plt.title("2023 Pesticide Use by Country (Top 15)")
plt.ylabel("Use (Total Value)")
plt.show()


# ydata_profiling

# !pip install ydata-profiling
# from ydata_profiling import ProfileReport
# profile_without_postharvest = ProfileReport(all_data_without_postharvest,
#                         title="Pesticide uses Analysis",
#                         explorative=True)

# profile_with_postharvest = ProfileReport(complete_data_with_postharvest_loss,
#                         title="Pesticide & Postharvest Loss Analysis",
#                         explorative=True)
# # Save it as an HTML file
# profile_without_postharvest.to_file("Pesticide_analysis_report.html")
# profile_with_postharvest.to_file("Pesticide_&_Postharvest_analysis_report.html")


# Machine Learning Analysis for Global Pesticide Use and Postharvest Loss

import os
import joblib
import warnings
warnings.filterwarnings("ignore")
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import RobustScaler, LabelEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
import xgboost as xgb

# Directories
REPORT_DIR = "reports"
MODEL_DIR = "models"
os.makedirs(REPORT_DIR, exist_ok=True)
os.makedirs(MODEL_DIR, exist_ok=True)


class PesticideFoodLossMLPipeline:
    def __init__(self):
        self.models = {}
        self.results = {}
        self.scaler = RobustScaler()
        self.label_encoders = {}
        self.feature_names = []
        self.col_map = {
            # common variants -> canonical names used in pipeline
            "Year": "year",
            "YEAR": "year",
            "Year_": "year",
            "Region": "region",
            "Sub-region Name": "region",
            "Item": "item",
            "Pesticides (total)": "pesticide_use",
            "Pesticide total": "pesticide_use",
            "value": "pesticide_use",
            "Food loss percentage": "food_loss_percentage",
            "Food loss %": "food_loss_percentage"
        }

    def _standardize_columns(self, df):
        df = df.copy()
        # rename known variants
        rename_map = {col: self.col_map[col] for col in df.columns if col in self.col_map}
        df = df.rename(columns=rename_map)
        # enforce lowercase only for recognized cols
        df.columns = [c if c in self.col_map.values() else c for c in df.columns]
        return df

    def load_and_explore_data(self, filepath):
        print("=== DATA LOADING AND EXPLORATION ===")
        df = pd.read_csv(filepath)
        df = self._standardize_columns(df)
        print(f"Dataset shape: {df.shape}")
        print("Columns:", df.columns.tolist())
        print("Missing values (top 10):")
        print(df.isna().sum().sort_values(ascending=False).head(10))
        return df

    def advanced_feature_engineering(self, df, dropna=True):
        print("\n=== ADVANCED FEATURE ENGINEERING ===")
        df_eng = df.copy()

        # Ensure canonical column names present
        required = ['year', 'region', 'item', 'pesticide_use', 'food_loss_percentage']
        missing = [c for c in required if c not in df_eng.columns]
        if missing:
            raise ValueError(f"Missing required columns: {missing}. Please check your CSV or column mapping.")

        # Optionally drop rows with missing required fields
        if dropna:
            before = df_eng.shape[0]
            df_eng = df_eng.dropna(subset=required)
            print(f"Dropped {before - df_eng.shape[0]} rows with missing key fields")

        # Numeric conversions
        df_eng['year'] = pd.to_numeric(df_eng['year'], errors='coerce').astype(int)
        df_eng['pesticide_use'] = pd.to_numeric(df_eng['pesticide_use'], errors='coerce').astype(float)
        df_eng['food_loss_percentage'] = pd.to_numeric(df_eng['food_loss_percentage'], errors='coerce').astype(float)

        # Core engineered features
        df_eng['efficiency_ratio'] = df_eng['food_loss_percentage'] / (df_eng['pesticide_use'] + 1)
        df_eng['pesticide_quartile'] = pd.qcut(df_eng['pesticide_use'].rank(method='first'), q=4, labels=[1, 2, 3, 4])
        df_eng['years_since_start'] = df_eng['year'] - df_eng['year'].min()
        df_eng['decade'] = (df_eng['year'] // 10) * 10
        df_eng['is_recent'] = (df_eng['year'] >= 2015).astype(int)
        df_eng['is_covid_era'] = (df_eng['year'] >= 2020).astype(int)

        # Log transforms for skew
        df_eng['log_pesticide_use'] = np.log1p(df_eng['pesticide_use'])
        df_eng['log_food_loss'] = np.log1p(df_eng['food_loss_percentage'])

        # Label encode region & item, store encoders
        le_r, le_i = LabelEncoder(), LabelEncoder()
        df_eng['region_encoded'] = le_r.fit_transform(df_eng['region'].astype(str))
        df_eng['item_encoded'] = le_i.fit_transform(df_eng['item'].astype(str))
        self.label_encoders['region'] = le_r
        self.label_encoders['item'] = le_i

        # Interaction terms
        df_eng['region_year_interaction'] = df_eng['region_encoded'] * df_eng['years_since_start']
        df_eng['item_pesticide_interaction'] = df_eng['item_encoded'] * df_eng['log_pesticide_use']

        print(f"Engineered features. New shape: {df_eng.shape}")
        return df_eng

    def prepare_features_target(self, df_eng):
        print("\n=== FEATURE PREPARATION ===")
        feature_columns = [
            'year', 'region_encoded', 'item_encoded', 'pesticide_use',
            'years_since_start', 'is_recent', 'is_covid_era',
            'log_pesticide_use', 'region_year_interaction',
            'item_pesticide_interaction', 'pesticide_quartile'
        ]
        available_features = [f for f in feature_columns if f in df_eng.columns]
        X = df_eng[available_features].copy()

        # handle categorical quartile
        if 'pesticide_quartile' in X.columns:
            X['pesticide_quartile'] = pd.to_numeric(X['pesticide_quartile'], errors='coerce').fillna(0).astype(int)

        # One-hot minimal
        X = pd.get_dummies(X, drop_first=True)
        self.feature_names = list(X.columns)
        y = df_eng['food_loss_percentage'].copy()

        print(f"Final X shape: {X.shape}, y shape: {y.shape}")
        return X, y

    def train_multiple_models(self, X, y, test_size=0.2, random_state=42):
        print("\n=== MODEL TRAINING & EVALUATION ===")
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)

        model_configs = {
            'Linear Regression': {'model': LinearRegression(), 'scaled': True, 'params': {}},
            'Ridge': {'model': Ridge(), 'scaled': True, 'params': {'alpha': [0.1, 1.0, 10.0]}},
            'Lasso': {'model': Lasso(max_iter=5000), 'scaled': True, 'params': {'alpha': [0.01, 0.1, 1.0]}},
            'Random Forest': {'model': RandomForestRegressor(random_state=42, n_jobs=-1), 'scaled': False,
                              'params': {'n_estimators': [100, 200], 'max_depth': [10, 20, None]}},
            'Gradient Boosting': {'model': GradientBoostingRegressor(random_state=42), 'scaled': False,
                                  'params': {'n_estimators': [100, 200], 'learning_rate': [0.05, 0.1], 'max_depth': [3, 5]}},
            'XGBoost': {'model': xgb.XGBRegressor(random_state=42, n_jobs=-1), 'scaled': False,
                        'params': {'n_estimators': [100, 200], 'learning_rate': [0.05, 0.1], 'max_depth': [3, 5]}}
        }

        for name, cfg in model_configs.items():
            print(f"\n--- {name} ---")
            model, use_scaled, params = cfg['model'], cfg['scaled'], cfg['params']
            Xtr, Xte = (X_train_scaled, X_test_scaled) if use_scaled else (X_train, X_test)

            if params:
                grid = GridSearchCV(model, params, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=0)
                grid.fit(Xtr, y_train)
                best = grid.best_estimator_
                print("Best params:", grid.best_params_)
            else:
                best = model.fit(Xtr, y_train)

            y_pred_train, y_pred_test = best.predict(Xtr), best.predict(Xte)

            cv_scores = cross_val_score(best, Xtr, y_train, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)
            cv_rmse = np.sqrt(-cv_scores.mean())
            cv_rmse_std = np.std(np.sqrt(-cv_scores))

            metrics = {
                'train_r2': r2_score(y_train, y_pred_train),
                'test_r2': r2_score(y_test, y_pred_test),
                'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train)),
                'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test)),
                'train_mae': mean_absolute_error(y_train, y_pred_train),
                'test_mae': mean_absolute_error(y_test, y_pred_test),
                'cv_rmse': cv_rmse,
                'cv_rmse_std': cv_rmse_std,
                'pred_test': y_pred_test,
                'actual_test': y_test.values
            }

            self.models[name] = best
            self.results[name] = metrics

            # save model
            joblib.dump(best, os.path.join(MODEL_DIR, f"{name.replace(' ', '_')}.joblib"))

            # save predictions
            pd.DataFrame({'actual_test': y_test.values, 'pred_test': y_pred_test}).to_csv(
                os.path.join(REPORT_DIR, f"predictions_{name.replace(' ', '_')}.csv"), index=False)

            # feature importance if available
            if hasattr(best, "feature_importances_"):
                fi_df = pd.DataFrame({'feature': self.feature_names, 'importance': best.feature_importances_})
                fi_df = fi_df.sort_values('importance', ascending=False)
                fi_df.to_csv(os.path.join(REPORT_DIR, f"feature_importance_{name.replace(' ', '_')}.csv"), index=False)

        # Save all metrics summary
        pd.DataFrame(self.results).T.to_csv(os.path.join(REPORT_DIR, "ml_model_results.csv"))
        print("Saved all model metrics to ml_model_results.csv")

        return X_train, X_test, y_train, y_test

    def create_comprehensive_visualizations(self):
        print("\n=== CREATING VISUALIZATIONS ===")
        if not self.results:
            print("No results to visualize.")
            return

        models = list(self.results.keys())
        test_r2 = [self.results[m]['test_r2'] for m in models]
        test_rmse = [self.results[m]['test_rmse'] for m in models]
        cv_rmse = [self.results[m]['cv_rmse'] for m in models]
        cv_std = [self.results[m]['cv_rmse_std'] for m in models]

        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        axes[0, 0].bar(models, test_r2); axes[0, 0].set_title("Test R2"); axes[0, 0].tick_params(axis='x', rotation=45)
        axes[0, 1].bar(models, test_rmse); axes[0, 1].set_title("Test RMSE"); axes[0, 1].tick_params(axis='x', rotation=45)
        axes[1, 0].bar(models, cv_rmse, yerr=cv_std, capsize=5); axes[1, 0].set_title("CV RMSE"); axes[1, 0].tick_params(axis='x', rotation=45)

        # best model actual vs predicted
        best = min(models, key=lambda m: self.results[m]['test_rmse'])
        res = self.results[best]
        axes[1, 1].scatter(res['actual_test'], res['pred_test'], alpha=0.6)
        mn, mx = min(res['actual_test']), max(res['actual_test'])
        axes[1, 1].plot([mn, mx], [mn, mx], 'r--')
        axes[1, 1].set_title(f"Actual vs Predicted ({best})")
        axes[1, 1].set_xlabel("Actual")
        axes[1, 1].set_ylabel("Predicted")

        plt.tight_layout()
        plt.savefig(os.path.join(REPORT_DIR, "model_comparison.png"), dpi=200)
        plt.show()
        print(f"Saved model comparison to {REPORT_DIR}/model_comparison.png")

    def run_shap_analysis(self, model_name):
        try:
            import shap
        except Exception:
            print("âš ï¸ shap not installed. Run: pip install shap")
            return

        if model_name not in self.models:
            print(f"Model {model_name} not found.")
            return

        model = self.models[model_name]
        if not hasattr(model, "feature_importances_"):
            print("SHAP works best with tree-based models.")
        print("For SHAP, re-run training and keep X_test handy to pass here.")

    def generate_insights_report(self):
        print("\n=== INSIGHTS & RECOMMENDATIONS ===")
        if not self.results:
            print("No results available.")
            return
        best = min(self.results.keys(), key=lambda m: self.results[m]['test_rmse'])
        print("Best model:", best)
        print(self.results[best])

        summary = []
        for name, r in self.results.items():
            summary.append({
                'model': name,
                'test_r2': r['test_r2'],
                'test_rmse': r['test_rmse'],
                'test_mae': r['test_mae'],
                'cv_rmse': r['cv_rmse']
            })
        pd.DataFrame(summary).to_csv(os.path.join(REPORT_DIR, "model_summary.csv"), index=False)
        print(f"Saved model summary to {REPORT_DIR}/model_summary.csv")


# Main runner
def run_complete_pipeline(filepath):
    pipeline = PesticideFoodLossMLPipeline()
    df = pipeline.load_and_explore_data(filepath)
    df_eng = pipeline.advanced_feature_engineering(df)
    X, y = pipeline.prepare_features_target(df_eng)
    pipeline.train_multiple_models(X, y)
    pipeline.create_comprehensive_visualizations()
    pipeline.generate_insights_report()
    return pipeline


if __name__ == "__main__":
    DATA_PATH = "complete_data_with_postharvest_loss.csv"
    pipeline = run_complete_pipeline(DATA_PATH)
    print("âœ… Done. Check the 'reports/' and 'models/' directories.")



# Statistical analysis of the efficiency paradox
from scipy import stats

# Load your final dataset
df = pd.read_csv('complete_data_with_postharvest_loss.csv')

# Test correlation significance
correlation, p_value = stats.spearmanr(df['value'], df['Food loss percentage'])
print(f"Spearman correlation: {correlation:.4f}, p-value: {p_value:.4f}")

# Regional ANOVA test
from scipy.stats import f_oneway
regions = df['Sub-region Name'].unique()
food_loss_by_region = [df[df['Sub-region Name'] == region]['Food loss percentage']
                       for region in regions]
f_stat, p_val = f_oneway(*food_loss_by_region)
print(f"Regional differences F-test: F={f_stat:.3f}, p={p_val:.3f}")

print("\n" + "="*60)
print("STATISTICAL SIGNIFICANCE ANALYSIS")
print("="*60)

# Statistical analysis of the efficiency paradox
from scipy import stats

# Load your final dataset
df = pd.read_csv('complete_data_with_postharvest_loss.csv')

# Test correlation significance
correlation, p_value = stats.spearmanr(df['value'], df['Food loss percentage'])
print(f"\nSpearman correlation (Pesticide vs Food Loss): {correlation:.4f}")
print(f"P-value: {p_value:.4f}")

if correlation > 0 and p_value < 0.05:
    print("ðŸš¨ EFFICIENCY PARADOX CONFIRMED!")
    print("Higher pesticide use correlates with MORE food loss")
elif correlation < 0 and p_value < 0.05:
    print("âœ… Expected relationship confirmed")
    print("Higher pesticide use correlates with LESS food loss")
else:
    print("ðŸ“Š No significant correlation found")
    print("Pesticide use and food loss appear independent")

# Regional efficiency analysis
regional_efficiency = df.groupby('Sub-region Name').agg({
    'value': 'mean',
    'Food loss percentage': 'mean'
}).round(3)

print(f"\nREGIONAL EFFICIENCY SUMMARY:")
print(regional_efficiency.head(10))

# Save the statistical results
results_summary = {
    'correlation_coefficient': correlation,
    'p_value': p_value,
    'significance': 'Yes' if p_value < 0.05 else 'No',
    'relationship': 'Positive' if correlation > 0 else 'Negative' if correlation < 0 else 'None'
}

with open('reports/statistical_results.txt', 'w') as f:
    f.write("STATISTICAL ANALYSIS RESULTS\n")
    f.write("="*30 + "\n")
    for key, value in results_summary.items():
        f.write(f"{key}: {value}\n")

print(f"\nðŸ“Š Statistical results saved to reports/statistical_results.txt")
print(f"\nðŸ“Š Statistical results saved to reports/statistical_results.txt")

# ADD THE NEW CODE RIGHT HERE:
# Ensure reports directory exists
os.makedirs('reports', exist_ok=True)

# Create comprehensive conclusion
print("\nðŸŽ“ FINAL SUMMARY:")
print("="*50)

if correlation > 0 and p_value < 0.05:
    conclusion = "EFFICIENCY PARADOX DISCOVERED"
    implication = "This challenges conventional agricultural wisdom"
elif correlation < 0 and p_value < 0.05:
    conclusion = "EXPECTED RELATIONSHIP CONFIRMED"
    implication = "Results align with traditional agricultural theory"
else:
    conclusion = "COMPLEX RELATIONSHIP IDENTIFIED"
    implication = "Food loss has multiple drivers beyond pesticide use"

print(f"KEY FINDING: {conclusion}")
print(f"IMPLICATION: {implication}")
print(f"STATISTICAL EVIDENCE: r={correlation:.3f}, p={p_value:.3f}")

# Save final summary
with open('reports/Summary.txt', 'w') as f:
    f.write("GLOBAL PESTICIDE USE AND FOOD LOSS ANALYSIS\n")
    f.write("="*50 + "\n\n")
    f.write(f"KEY FINDING: {conclusion}\n")
    f.write(f"STATISTICAL EVIDENCE: r={correlation:.3f}, p={p_value:.3f}\n")
    f.write(f"SAMPLE SIZE: {len(df)} observations\n")
    f.write(f"TIME PERIOD: {df['Year'].min()}-{df['Year'].max()}\n")
    f.write(f"REGIONS ANALYZED: {len(df['Sub-region Name'].unique())}\n")

print("ðŸ“ Check 'reports/' folder for all outputs")